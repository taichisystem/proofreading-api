{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a8061f",
   "metadata": {},
   "source": [
    "# BERTによる自然言語処理入門\n",
    "## 第9章参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edda075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-1\n",
    "!mkdir chap9\n",
    "%cd ./chap9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6109fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-2\n",
    "!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-3\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 日本語の事前学習済みモデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-4\n",
    "class SC_tokenizer(BertJapaneseTokenizer):\n",
    "       \n",
    "    def encode_plus_tagged(\n",
    "        self, wrong_text, correct_text, max_length=128\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ファインチューニング時に使用。\n",
    "        誤変換を含む文章と正しい文章を入力とし、\n",
    "        符号化を行いBERTに入力できる形式にする。\n",
    "        \"\"\"\n",
    "        # 誤変換した文章をトークン化し、符号化\n",
    "        encoding = self(\n",
    "            wrong_text, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True\n",
    "        )\n",
    "        # 正しい文章をトークン化し、符号化\n",
    "        encoding_correct = self(\n",
    "            correct_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        ) \n",
    "        # 正しい文章の符号をラベルとする\n",
    "        encoding['labels'] = encoding_correct['input_ids'] \n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(\n",
    "        self, text, max_length=None, return_tensors=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = [] # トークンを追加していく。\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([\n",
    "                    token.replace('##','') for token in tokens_word\n",
    "                ])\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = self.convert_tokens_to_ids(tokens) \n",
    "        encoding = self.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length' if max_length else False, \n",
    "            truncation=True if max_length else False\n",
    "        )\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == 'pt':\n",
    "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    def convert_bert_output_to_text(self, text, labels, spans):\n",
    "        \"\"\"\n",
    "        推論時に使用。\n",
    "        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n",
    "        そこから、BERTによって予測された文章に変換。\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(labels)\n",
    "\n",
    "        # labels, spansから特殊トークンに対応する部分を取り除く\n",
    "        labels = [label for label, span in zip(labels, spans) if span[0]!=-1]\n",
    "        spans = [span for span in spans if span[0]!=-1]\n",
    "\n",
    "        # BERTが予測した文章を作成\n",
    "        predicted_text = ''\n",
    "        position = 0\n",
    "        for label, span in zip(labels, spans):\n",
    "            start, end = span\n",
    "            if position != start: # 空白の処理\n",
    "                predicted_text += text[position:start]\n",
    "            predicted_token = self.convert_ids_to_tokens(label)\n",
    "            predicted_token = predicted_token.replace('##', '')\n",
    "            predicted_token = unicodedata.normalize(\n",
    "                'NFKC', predicted_token\n",
    "            ) \n",
    "            predicted_text += predicted_token\n",
    "            position = end\n",
    "        \n",
    "        return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-5\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-6\n",
    "wrong_text = '優勝トロフィーを変換した'\n",
    "correct_text = '優勝トロフィーを返還した'\n",
    "encoding = tokenizer.encode_plus_tagged(\n",
    "    wrong_text, correct_text, max_length=12\n",
    ")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-7\n",
    "wrong_text = '優勝トロフィーを変換した'\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    wrong_text, return_tensors='pt'\n",
    ")\n",
    "print('# encoding')\n",
    "print(encoding)\n",
    "print('# spans')\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-8\n",
    "predicted_labels = [2, 759, 18204, 11, 8274, 15, 10, 3]\n",
    "predicted_text = tokenizer.convert_bert_output_to_text(\n",
    "    wrong_text, predicted_labels, spans\n",
    ")\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-9\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "bert_mlm = bert_mlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-10\n",
    "text = '優勝トロフィーを変換した。'\n",
    "\n",
    "# 符号化とともに各トークンの文章中の位置を計算しておく。\n",
    "encoding, spans = tokenizer.encode_plus_untagged(\n",
    "    text, return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "# BERTに入力し、トークン毎にスコアの最も高いトークンのIDを予測値とする。\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(**encoding)\n",
    "    scores = output.logits\n",
    "    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "    \n",
    "# ラベル列を文章に変換\n",
    "predict_text = tokenizer.convert_bert_output_to_text(\n",
    "    text, labels_predicted, spans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c975a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-11\n",
    "data = [\n",
    "    {\n",
    "        'wrong_text': '優勝トロフィーを変換した。',\n",
    "        'correct_text': '優勝トロフィーを返還した。',\n",
    "    },\n",
    "    {\n",
    "        'wrong_text': '人と森は強制している。',\n",
    "        'correct_text': '人と森は共生している。',\n",
    "    }\n",
    "]\n",
    "\n",
    "# 各データを符号化し、データローダへ入力できるようにする。\n",
    "max_length=32\n",
    "dataset_for_loader = []\n",
    "for sample in data:\n",
    "    wrong_text = sample['wrong_text']\n",
    "    correct_text = sample['correct_text']\n",
    "    encoding = tokenizer.encode_plus_tagged(\n",
    "        wrong_text, correct_text, max_length=max_length\n",
    "    )\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)\n",
    "\n",
    "# データローダを作成\n",
    "dataloader = DataLoader(dataset_for_loader, batch_size=2)\n",
    "\n",
    "# ミニバッチをBERTへ入力し、損失を計算。\n",
    "for batch in dataloader:\n",
    "    encoding = { k: v.cuda() for k, v in batch.items() }\n",
    "    output = bert_mlm(**encoding)\n",
    "    loss = output.loss # 損失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-12\n",
    "!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n",
    "!tar zxvf JWTD.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-13\n",
    "def create_dataset(data_df):\n",
    "\n",
    "    tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def check_token_count(row):\n",
    "        \"\"\"\n",
    "        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n",
    "        （条件は上の文章を参照）\n",
    "        \"\"\"\n",
    "        wrong_text_tokens = tokenizer.tokenize(row['wrong_text'])\n",
    "        correct_text_tokens = tokenizer.tokenize(row['correct_text'])\n",
    "        if len(wrong_text_tokens) != len(correct_text_tokens):\n",
    "            return False\n",
    "        \n",
    "        diff_count = 0\n",
    "        threthold_count = 2\n",
    "        for wrong_text_token, correct_text_token \\\n",
    "            in zip(wrong_text_tokens, correct_text_tokens):\n",
    "\n",
    "            if wrong_text_token != correct_text_token:\n",
    "                diff_count += 1\n",
    "                if diff_count > threthold_count:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def normalize(text):\n",
    "        \"\"\"\n",
    "        文字列の正規化\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        return text\n",
    "\n",
    "    # 漢字の誤変換のデータのみを抜き出す。\n",
    "    category_type = 'kanji-conversion'\n",
    "    data_df.query('category == @category_type', inplace=True) \n",
    "    data_df.rename(\n",
    "        columns={'pre_text': 'wrong_text', 'post_text': 'correct_text'}, \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    # 誤変換と正しい文章をそれぞれ正規化し、\n",
    "    # それらの間でトークン列に対応がつくもののみを抜き出す。\n",
    "    data_df['wrong_text'] = data_df['wrong_text'].map(normalize) \n",
    "    data_df['correct_text'] = data_df['correct_text'].map(normalize)\n",
    "    kanji_conversion_num = len(data_df)\n",
    "    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n",
    "    same_tokens_count_num = len(data_df)\n",
    "    print(\n",
    "        f'- 漢字誤変換の総数：{kanji_conversion_num}',\n",
    "        f'- トークンの対応関係のつく文章の総数: {same_tokens_count_num}',\n",
    "        f'  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)',\n",
    "        sep = '\\n'\n",
    "    )\n",
    "    return data_df[['wrong_text', 'correct_text']].to_dict(orient='records')\n",
    "\n",
    "# データのロード\n",
    "train_df = pd.read_json(\n",
    "    './jwtd/train.jsonl', orient='records', lines=True\n",
    ")\n",
    "test_df = pd.read_json(\n",
    "    './jwtd/test.jsonl', orient='records', lines=True\n",
    ")\n",
    "\n",
    "# 学習用と検証用データ\n",
    "print('学習と検証用のデータセット：')\n",
    "dataset = create_dataset(train_df)\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.8)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:]\n",
    "\n",
    "# テストデータ\n",
    "print('テスト用のデータセット：')\n",
    "dataset_test = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-14\n",
    "def create_dataset_for_loader(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力可能な形式にする。\n",
    "    \"\"\"\n",
    "    dataset_for_loader = []\n",
    "    for sample in tqdm(dataset):\n",
    "        wrong_text = sample['wrong_text']\n",
    "        correct_text = sample['correct_text']\n",
    "        encoding = tokenizer.encode_plus_tagged(\n",
    "            wrong_text, correct_text, max_length=max_length\n",
    "        )\n",
    "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# データセットの作成\n",
    "max_length = 32\n",
    "dataset_train_for_loader = create_dataset_for_loader(\n",
    "    tokenizer, dataset_train, max_length\n",
    ")\n",
    "dataset_val_for_loader = create_dataset_for_loader(\n",
    "    tokenizer, dataset_val, max_length\n",
    ")\n",
    "\n",
    "# データローダの作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train_for_loader, batch_size=32, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-15\n",
    "class BertForMaskedLM_pl(pl.LightningModule):\n",
    "        \n",
    "    def __init__(self, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_mlm(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_mlm(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "\n",
    "# ファインチューニング\n",
    "model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-16\n",
    "def predict(text, tokenizer, bert_mlm):\n",
    "    \"\"\"\n",
    "    文章を入力として受け、BERTが予測した文章を出力\n",
    "    \"\"\"\n",
    "    # 符号化\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(\n",
    "        text, return_tensors='pt'\n",
    "    ) \n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "\n",
    "    # ラベルの予測値の計算\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "\n",
    "    # ラベル列を文章に変換\n",
    "    predict_text = tokenizer.convert_bert_output_to_text(\n",
    "        text, labels_predicted, spans\n",
    "    )\n",
    "\n",
    "    return predict_text\n",
    "\n",
    "# いくつかの例に対してBERTによる文章校正を行ってみる。\n",
    "text_list = [\n",
    "    'ユーザーの試行に合わせた楽曲を配信する。',\n",
    "    'メールに明日の会議の史料を添付した。',\n",
    "    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n",
    "    '突然、子供が帰省を発した。'\n",
    "]\n",
    "\n",
    "# トークナイザ、ファインチューニング済みのモデルのロード\n",
    "tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n",
    "bert_mlm = model.bert_mlm.cuda()\n",
    "\n",
    "for text in text_list:\n",
    "    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n",
    "    print('---')\n",
    "    print(f'入力：{text}')\n",
    "    print(f'出力：{predict_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
